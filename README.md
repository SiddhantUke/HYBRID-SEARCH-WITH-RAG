# ğŸ§  Hybrid Search with RAG (Retrieval-Augmented Generation)


> ğŸš€ A project demonstrating **Hybrid Search** â€” combining **semantic embeddings** and **keyword-based BM25 retrieval** using **LangChain**, **Pinecone**, and **Hugging Face**.  
> This serves as the **retrieval backbone** for modern **RAG (Retrieval-Augmented Generation)** systems.

---

## ğŸ“˜ Overview

This project showcases how to implement a **hybrid search mechanism** that merges the best of both retrieval methods:

- **Dense retrieval** for semantic understanding using embeddings  
- **Sparse retrieval** for exact keyword matching using BM25  

By combining them, the system delivers **more accurate and context-aware results**, forming the foundation for **RAG-powered AI applications** such as chatbots, knowledge assistants, and Q&A systems.

---

## ğŸ§© Project Workflow

1. **Environment Setup**
   - Install and import all required dependencies.
   - Initialize Pinecone client using your API key.
   - Create a vector index for hybrid search.

2. **Embedding Models**
   - Dense Embeddings â†’ `HuggingFaceEmbeddings(all-MiniLM-L6-v2)`
   - Sparse Embeddings â†’ `BM25Encoder()` from `pinecone-text`

3. **Hybrid Retriever Creation**
   - Combine both embedding types using `PineconeHybridSearchRetriever` from LangChain.

4. **Data Ingestion**
   ```python
   sentences = [
       "In 2023, I watched Attack on Titan: The Final Season",
       "In 2022, I watched Demon Slayer: Entertainment District Arc",
       "In 2021, I watched Jujutsu Kaisen"
   ]
   retriever.add_texts(sentences)




5.  **Hybrid Search Query**

retriever.invoke("Which anime did I watch in 2022?")


âœ… Output:
In 2022, I watched Demon Slayer: Entertainment District Arc




âš™ï¸ **Tech Stack**

Technology	               Purpose
Python	                   Programming Language
LangChain                  Framework for retrievers & chaining LLM components
Pinecone                 	 Vector database for dense & sparse retrieval
Hugging Face               Transformers	Generate semantic embeddings
BM25 Encoder               (Pinecone Text)	Perform keyword-based retrieval
dotenv	                   Manage environment variables securely




**ğŸ§  What is RAG?**

Retrieval-Augmented Generation (RAG) combines:

Retriever â†’ fetches relevant documents using hybrid search

Generator â†’ an LLM (like GPT, LLaMA, or Gemini) that uses retrieved data to generate accurate, grounded answers

This project focuses on building the retrieval layer â€” the most critical part of any RAG pipeline.





**ğŸ” Key Features**

âœ… Combines semantic and keyword-based retrieval

âš¡ Uses Pinecone for scalable, low-latency vector search

ğŸ§  Integrates Hugging Face embeddings (all-MiniLM-L6-v2)

ğŸ”§ Implements BM25 sparse retrieval for keyword precision

ğŸ’¬ Forms the foundation of a RAG-based AI system




**ğŸ§± Setup Instructions**




1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/hybrid-search-with-rag.git
cd hybrid-search-with-rag



2ï¸âƒ£ Install Dependencies
pip install --upgrade pinecone-client pinecone-text langchain_community langchain_huggingface



3ï¸âƒ£ Add Environment Variables

Create a .env file and add your Pinecone API key:
PINECONE_API_KEY=your_api_key_here



4ï¸âƒ£ Run the Script
python hybrid_search_with_rag_.py



ğŸ§¾ Example Output
Query: Which anime did I watch in 2022?
Result: In 2022, I watched Demon Slayer: Entertainment District Arc



ğŸ’¡ Use Cases

ğŸ¤– Chatbots with contextual memory

ğŸ§¾ Document Question Answering

ğŸ§  AI-powered search systems

ğŸ“š Knowledge base augmentation for LLMs

ğŸ” Semantic + keyword search applications

ğŸš€ Future Enhancements

Integrate with LLMs (GPT, Gemini, LLaMA) for full RAG pipeline

Add document chunking and metadata filtering

Implement hybrid reranking using cross-encoders

Extend retrieval to multi-modal (text + image) data




ğŸ“š Learning Outcomes

Through this project, youâ€™ll gain hands-on understanding of:

Vector databases and hybrid search

Dense vs. sparse retrieval methods

Embedding models and BM25 encoding

RAG architecture fundamentals

Real-world AI search pipelines





ğŸ¤ Contributing

Contributions are welcome!
If youâ€™d like to improve this project or extend it into a full RAG pipeline, feel free to fork and submit a pull request.




ğŸ“„ License

This project is licensed under the MIT License â€“ free to use and modify.




ğŸ§‘â€ğŸ’» Author

AI Engineer | Building Intelligent Systems with LLMs, RAG, and Hybrid Retrieval




ğŸŒ Connect With Me

ğŸ’¼ LinkedIn - www.linkedin.com/in/siddhant-uke

ğŸ™ GitHub - https://github.com/SiddhantUke

